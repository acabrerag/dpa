#+Title:     Apache Hadoop
#+Author:    Adolfo De Unánue
#+Email:     adolfo.deunanue@itam.mx
#+DATE:      2017
#+DESCRIPTION: General discussion about the issues to be solve in order to build the product
#+KEYWORDS:  datank product 
#+LANGUAGE:  en

#+STARTUP: beamer
#+STARUP: oddeven

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [presentation, smaller]

#+BEAMER_THEME: DarkConsole

#+OPTIONS: H:1 toc:nil 

#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)


* TODO Apache Hadoop




* Docker
:PROPERTIES:
   :reveal_background: #000fff
:END:

** Obtener la imagen

#+begin_src shell :eval never

docker pull nanounanue/docker-hadoop
#+end_src


** Ejecutar un contenedor

#+begin_src shell :eval never
docker run -ti --rm \
  -v /ruta/a/tus/datos/:/home/itam/data \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
  -p 8000:8000 \
nanounanue/docker-hadoop /bin/zsh

#+end_src


** Contenedor con Hadoop

#+begin_src shell :eval never

docker run -ti --name hadoop-pseudo \
  -v /ruta/a/tus/datos/:/home/itam/data \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
  -p 8000:8000 -p 9999:9999 \
nanounanue/docker-hadoop

#+end_src


** Reiniciar el contenedor

#+begin_src shell :eval never

docker start -ai hadoop-pseudo

#+end_src


** Conectarse a un contenedor funcionando

- Esto podría ser útil para ver, por ejemplo =logs= o arrancar varios clientes.

- Averigua el número del contenedor

#+begin_src shell :eval never

docker ps -a

#+end_src



#+begin_src shell :eval never
docker exec -it <CONTENEDOR_ID> /bin/zsh

#+end_src


o en nuestro caso:

#+begin_src shell :eval never
docker exec -it hadoop_pseudo /bin/zsh

#+end_src


- *Nota*: /Recuerda que con los primeros 4 dígitos del contenedor basta para identificarlo./



** Navegador Web

- [[http://127.0.0.1:50090][Consola de Yarn]]

- [[http://127.0.0.1:50070][Consola de HDFS]]

- [[http://0.0.0.0:8000][HUE - Hadoop User Experience]]
  - /Desactivado/


* Apache Hadoop

* ¿Por qué?

- Aunque la capacidad de los discos ha aumentado considerablemente, la velocidad de los mismos no lo ha hecho igual.
  - Los discos actuales de =1 Tb=, tardan en leerse completos a =100 Mb/s= cerca de dos a tres horas.
  - Podemos /paralelizar/ las fuentes en varios discos.
    - Para leerla simultáneamente
  - Con varios discos, la **probabilidad de falla** aumenta.
- Otro problema es la distribución ¿Cómo combinas varios =file systems=?

* ¿Qué es?

- Sistema confiable (/realiable/) de almacenamiento compartido y de procesamiento de datos.
  - *Almacenamiento*: /Hadoop Distributed File System/, =HDFS=
  - *Procesamiento*: Varios /frameworks/ basados en =YARN=.

- Puede procesar cantidades masivas de datos y escalar conforme crezcan los datos.

- Flexibilidad para el procesamiento de datos.
  - No importa la estructura o falta de ella

- Está construido en =Java=.

* ¿Cómo?

- =MapReduce= es un sistema de procesamiento /batch/
  - Permite correr /queries/ contra **toda** tu base de datos
  - Pero el resultado puede tardar minutos, horas, etc...
  - No permite tener a un humano sentado ahí para retroalimentar.

* ¿Cómo?

- Ahora, gracias a =YARN= (ver más adelante) tenemos diferentes tipos de procesamiento:
  - /SQL Interactivo/: =Impala=, =Hive=, =Spark SQL=.
  - /Iterativos/: =Spark=.
  - /Procesamiento de flujos/: =Storm=, =Spark Streaming=.
  - /Búsquedas/: =Solr=.
  - /Grafos/ =Spark GraphX=.

* ¿Por qué no otros sistemas?

- ¿Por qué no usar un =PostgreSQL= con muchos discos, muy /pimpeado/?
  - El problema viene del tiempo que toma mover la cabeza del disco a otro lugar del disco para leer o escribir datos (/seek time/).
    - ¿Cuál es la /latencia/ de la operación?

- ¿Por qué no /Grid/?
  - Por ejemplo, cosas  de =HPC= que usan =MPI=.
    - Son intensivos en **CPU**.
  - Pero si hay que mover cientos de gigabytes, la transferencia de datos se vuelve un problema.
    - Basicamente, en que =Hadoop= opera con /data locality/.

* Componentes de Apache Hadoop

- *MapReduce* Modelo de procesamiento /batch/ de datos distribuido y paralelo.
- *HDFS* Sistema de archivos (/file system/) distribuido.
- *Pig* Capa de abstracción encima de =MapReduce=. Utiliza /Pig Latin/ un lenguaje de flujo de datos
  - Como =dplyr=
- *Hive* (Hadoop InteractiVE) Es un lenguaje parecido al =SQL=: =HQL=, para ejecutar /queries/ sobre el =HDFS=.
- *HBase* Base de datos distribuida orientada a columnas.
  - Depende de =Zookeeper=.
- *Impala* Lenguaje Interactivo parecido al =SQL=, pero mucho más rápido de =HIVE= debido a su arquitectura *MPP*.

* Componentes de Apache Hadoop

- *Zookeeper* Proyecto que proveé un servicio centralizado para facilitar la coordinación de componentes de Hadoop.
- *Sqoop* Herramienta para mover datos entre =RDBM= y =HDFS=.
- *Flume* Servicio para recolectar, agregar y mover grandes cantidades de datos entre máquinas individuales y el =HDFS=.
- *Oozie* Sistema de /workflow/, se usa para coordinar varios /jobs/ de *MapReduce*.
- *Mahout* Biblioteca de /Machine Learning/.
  - Ver la carpeta =docs=.
- *Ambari* Simplifica el aprovisionamiento, gestión y /monitoreo/ de un /cluster/ de Hadoop.
- *Avro* Formato de serialización y de persistencia de datos.
- Entre otros...



* HDFS : Hadoop File System

* HDFS

- Sistema de almacenamiento distribuido.
  - /Namenode/ =->= Master
  - /Datanode/ =->=  Slaves

* /Schema on Read/

- Es posible cargar datos sin procesar dentro de Hadoop, la estructura se dará en el tiempo de procesamiento.

- Es muy diferente a /Schema on Write/ como el usado en los =RDBM=s
  - /Schema on Write/ impone un ciclo de análisis y modelado de datos, así como de su transformación, carga y prueba, antes de los datos puedan ser accesados.
  - Esto quita mucha flexibilidad: Si se tomaron decisiones incorrectas o los requerimientos cambian, es necesario empezar de nuevo =:(= .

* Ventajas

- Archivos muy grandes
- /write once, read many times/.
- Hardware /normal/

* Desventajas

- Acceso a los datos de baja latencia.
- Muchos archivos pequeños.
- Muchas escrituras, modificaciones

* Tamaño del bloque

- Cada /file system/ define un tamaño de bloque, el cual es la cantidad mínima de datos que puede escribir o leer.
  - Típicamente son de =kb=.
- En =HDFS=, el bloque es de =128 Mb= por /default/.
  - Es el concepto fundamental, no el archivo.


* /Namenode/
 - Gestiona el /filesystem/
   - Mantiene el árbol del /filesystem/.
   - Mantiene los =metadatos= de todos los archivos y carpetas del árbol.
   - Esta información se guarda en disco en dos archivos:
     - =namespace image=
     - =edit log=
 - Indica a los /datanodes/ realizar tareas de bajo nivel de =I/O=.
 - /Book Keeper/
   - División de archivos en bloques (¿Cómo?)
   - En qué /datanode/ (¿Quién?)
   - Monitorea.
 - Uso intensivo de =RAM= y de =I/O=.
 - Si se /cae/ el =HDFS= no puede ser usado
   - Hasta la versión =1.x= el /single point of failure/, en Hadoop 2 se incorporó la característica de /HIgh Availability/.
   - Su caída puede causar la pérdida total de los datos.



* Arquitectura: Ingesta de datos

* Decisiones Arquitectónicas

- El hecho de que el =HDFS= permita /Schema on Read/, no elimina la necesidad de tomar decisiones arquitectónicas en la ingesta de los datos, entre ellas:

  - ¿Cómo se guardarán los datos?
    - Capa de almacenamiento
    - Formatos de archivos
    - Formatos de compresión

  - ¿Diseño de esquema de datos?
    - Directorios donde guardar los datos y donde ponerlos luego del procesamiento y analítica.
    - También en =HBase= y en =Hive= se definen esquemas.
  - ¿Cómo se gestionarán los metadatos?
  - ¿Cómo se administrará la seguridad?
    - Autenticación, cifrado, acceso controlado.

* Capa de almacenamiento: =HDFS= vs =HBase=

- =HDFS=
  - Almacena los datos como archivos
  - /Scans/ rápidos.
  - Malo para acceso aleatorio en escritura y lectura.

- =HBase=
  -  Guarda los datos como archivos de HBase en el =HDFS=.
  - /Scans/ lentos.
  - Rápido acceso aleatorio a lectura y escritura.

* Capa de almacenamiento: =HDFS= vs =HBase=

En esta clase nos enfocaremos a =HDFS= y no a =HBase=.

* Formatos de archivos

- Tipos de archivos de Hadoop
  - Basados en archivos: =SequenceFiles=.
  - Formatos serializados: =Avro=, =Thrift=.
  - Formatos columnares: =RCFile=, =ORCFile=, =Parquet=.

- Debido a que la mayoría de formatos de archivos sólo se puede acceder desde =Java=, nos enfocaremos en sólo dos: =Avro= y =Parquet=
  - Además, ya casi no son usados los demás.

* Formatos de archivos: =Avro=
  - Independiente del lenguaje.
  - Almacena el esquema en el encabezado de cada archivo.
  - Son comprensibles y divisibles.
    - Soporta compresión con =snappy=.
  - Es recomendable usarlo en la ingesta de datos.
  - Las fallas sólo afectan a una porción del archivo.


* Formatos de archivos: =Parquet=
  - Diseñado para proveer procesamiento eficiente a través de varios compoentes de hadoop.
  - Almacena los datos de manera columnar.
  - Provee excelentes capacidades de compresión.
  - Soporta estructuras de datos complejas y anidadas.
  - Los metadatos están guardados al final del archivo.
  - Puede escribirse y leerse con las APIs de Avro y con esquemas de Avro.
  - No son tan buenos para recuperarse de errores.

* Formatos de compresión

- Ayuda a reducir los requerimientos de almacenamiento
- Mejora el procesamiento de los datos
  - Disminuye ,a cantidad de I/O en disco y red.
- Para aprovechar las capacidades de procesamiento en paralelo de Hadoop es preferible que el formato sea divisible.



* Formatos de compresión: =bzip2=
  - Excelente factor  de compresión
  - Pero muuuuuy lento en compresión/decompresión
  - Divisible

* Formatos de compresión: =snappy=
  - Proyecto de Google.
  - No es divisible, pero muy eficiente en compresión/decompresión.
  - Se debe de usar con un formato de archivo que provea la capacidad de contenedor (=Avro=, =SequenceFiles=).

* Formatos de compresión: =gzip=
  - No es divisible
  - Buen factor de compresión: 2.5x lo de =snappy=.
  - Se debe de usar con un formato de archivo que provea la capacidad de contenedor (=Avro=, =SequenceFiles=).

* Formatos de compresión: =lzop=
  - Parecido a =snappy= en eficiencia de compresión/decompresión.
  - Divisible, pero requiere una etapa de indexado.
  - Buena elección para guardar archivos de texto planos que no se pondrán dentro de un contenedor.
  - Licenciamiento raro (No viene incluido con Hadoop).

* Esquema

- *Nota*: /Basado en [[http://shop.oreilly.com/product/0636920033196.do][Hadoop Application Architectures.]]/

-  ¿Por qué?
   - Estructura de archivos estándar facilita la colaboración entre equipos.
   - Permite la reutilización de código para procesarla.
   - Permite reforzar las políticas de acceso y evitar así corrupción de los datos.
   - Permite identificar que datos han sido procesados completamente y cuales no
   - Muy parecido a los =schemas= de PostgreSQL.

* Esquema Propuesto (1/4)

- =/user/<username>=
   - Datos para experimentar (i.e. no son parte del proceso de negocio).
   - =JARs=, archivos de configuración.
   - Sólo debe de tener permisos de R/W el usuario en cuestión.

* Esquema Propuesto (2/4) 

- =/etl=
   - Datos en sus varias etapas de transformación por el ETL.
   - Subdirectorios reflejan el /workflow/ de los datos.
     - Los ETL son creados por *grupos* para *aplicaciones*.
     - Además cada subdirectorio tendrá a su vez directorios para cada etapa del proceso:
       - =input= para el lugar donde llegan los archivos
       - =procesando= para los pasos intermedios (puede haber varios)
       - =output= para el resultado final
       - =rechazados= para los registros o archivos que no pudieron ser procesados y que deben de verificarse manualmente.
   - La estructura quedaría así:
     - =/etl/<grupo>/<aplicación>/<proceso>/{input, procesando, output, rechazados}=
   - Sólo el usuario =etl= y los usuarios del grupo =etl= pueden R/W.

* Esquema Propuesto (3/4) 

- =/tmp=
   - Datos temporales generados por usuarios o partes de Hadoop.
   - Se borra su interior regularmente.
   - Todos tienen permisos de RW en este directorio.

- =/data=
   - Datos procesados y usados por la organización
   - Existen controles sobre quién puede o no usar los datos
   - Los usuarios sólo tienen permisos de lectura.
   - Los procesos automatizados (y auditados) tienen permisos de escritura.

* Esquema Propuesto (4/4) 

- =/app=
   - Todo lo requerido por la aplicación de Hadoop para funcionar (salvo datos)
   - Archivos de Oozie (definiciones de /workflows/),
   - Archivos de =hql=, =pig=, =JARs=, =UDFs=, etc.

* Otras consideraciones

- *Particionado*
  - Ayuda a reducir la cantidad de I/O para procesar los datos.
  - Es una especie de /indexado/ básico.

#+begin_src shell :eval never
<nombre del dataset>/<columna sobre la cual particionar>=<valor de la columna>/{archivos}
#+end_src

- *Denormalizar*
  - Ahorras =Joins= (que son lentos)

* Ejercicio I
- En este ejercicio prepararemos el *esquema* de nuestra aplicación de gran escala.
- Inicializa el contenedor =hadoop-pseudo=.
- Cambia al usuario =itam=.
- Revisa la estructura de directorios con el usuario =hdfs=.
  - Esto lo puedes hacer con =sudo -u hdfs ...=
- Crea el esquema de directorios propuesta.
  - Esto lo puedes hacer con =sudo -u hdfs ...=
  - =/user/<username>=, =/etl= (para la aplicación =ufo= y =gdelt=, el grupo es =ds=), =/tmp=, =/app= y =/data=.
  - Las últimas tres están vacías.

* Ejercicio I

- Asigna los permisos adecuados.
  - [[http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html][HDFS Permissions Guide]], [[http://stackoverflow.com/questions/23095244/add-new-group-to-hdfs][Pregunta de Stackoverflow]], [[http://spryinc.com/blog/hdfs-permissions-overcoming-permission-denied-accesscontrolexception][Problema del supergroup]]

- Dentro de tu carpeta (siendo el usuario =itam=), crea la carpeta =datasets=

- Dentro de tu carpeta (siendo el usuario =itam=), crea la carpeta =experimentos=.

- Carga dos archivos de cada dataset a esta carpeta desde =/home/itam/data/= usando la línea de comandos.
  - Observa que una de las carpetas es local...

- Verifiquemos que los datos estén bien:

#+begin_src shell :eval never
hadoop fs -cat experimentos/UFO-Dic-2014.tsv | wc -l
hadoop fs -cat experimentos/UFO-Dic-2014.tsv | head
#+end_src

- Observa como los datos están en formato de texto, justo como la copia que está en tu disco duro.


* Ejercicio II

- En este ejercicio usaremos =kite=.
- [[http://kitesdk.org/][=Kite=]] es una herramienta que nos permite cargar y administrar los metadatos de los archivos a Hadoop.
  - Pueden obtener ayuda con =kite-dataset help comando=.
- Tanto =Avro=, como =Hive Metastore= pueden servir para gestionar los metadatos y =kite= puede trabajar con ambos.
- En este ejercicio, nos enfocaremos en el dataset de =ufos=.
- Y a partir de aquí, todos los ejercicios son con el usuario =itam=.
- =HDFS= y =Avro= para guardar los metadatos.
- Infiere el esquema a partir de uno de los archivos:

#+begin_src shell :eval never
kite-dataset csv-schema data/UFO-Nov-2014.tsv --class UFO -o ufos.avsc  --delimiter "\t"
#+end_src

* Ejercicio II

- Esto va a marcar un error, arréglalo con =sed=.
  - Cuando hay =/= de por medio puedes cambiar el separador de =sed= por cualquier caracter, ejemplo:

#+begin_src shell :eval never
sed -e -i 's@cambiar_algo@por_esto@g' archivo
#+end_src


- Abre el archivo =ufos.avsc=, es el esquema en formato =avro=.
- Ahora crearemos el =dataset= en el =hdfs=.

#+begin_src shell :eval never
kite-dataset create dataset:hdfs:/user/itam/datasets/ufos --schema ufos.avsc
#+end_src

- Observa los cambios ocurridos en la carpeta =ufos= del =hdfs=.
  - Recuerda que puedes ver el contenido con el comando =hadoop fs -cat=

- Para verificar que se realizó bien puedes ejecutar:

#+begin_src shell :eval never
kite-dataset schema dataset:hdfs:/user/itam/datasets/ufos
#+end_src

* Ejercicio II

- Por último, importemos los datos

#+begin_src shell :eval never
kite-dataset csv-import data/UFO-Nov-2014.tsv dataset:hdfs:/user/itam/datasets/ufos --delimiter "\t"
kite-dataset csv-import data/UFO-Dic-2014.tsv dataset:hdfs:/user/itam/datasets/ufos --delimiter "\t"

#+end_src


- Veamos que si se copiaron:
#+begin_src shell :eval never
kite-dataset show dataset:hdfs:/user/itam/datasets/ufos
#+end_src

- Ahora observa como se ve un conjunto de datos en fornato =avro=, usando las herramientas de línea de comandos.
  - No lo abras con =hadoop fs -cat ...= o la consola se dañará...

- *NOTA*: Si algo salió mal, puedes borrar el dataset con
#+begin_src shell :eval never
kite-dataset delete dataset:hdfs:/user/itam/datasets/ufos

#+end_src

* Ejercicio II

- Ahora guardaremos los datos en  =hive metastore=.
  - No te preocupes más adelante explicaré que es esto, por el momento piensa en una base de datos para los metadatos.

- Los pasos son casi los mismos que el ejercicio anterior, sólo cambia el destino: ya no es el =HDFS=, ahora es =hive metastore=.

- Crea el =dataset=

#+begin_src shell :eval never
kite-dataset create ufos --schema ufos.avsc

#+end_src

- Para verificar que se realizó bien puedes ejecutar:

#+begin_src shell :eval never
kite-dataset schema ufos

#+end_src

- Y para asegurarnos que no son los mismos datos que antes (los guardados en el =hdfs=), ejecuta

#+begin_src shell :eval never
kite-dataset show ufos

#+end_src

* Ejercicio II

- Importemos los datos

#+begin_src shell :eval never
kite-dataset csv-import data/UFO-Nov-2014.tsv ufos --delimiter "\t"
kite-dataset csv-import data/UFO-Dic-2014.tsv ufos --delimiter "\t"

#+end_src


- Veamos que si se copiaron:
#+begin_src shell :eval never
kite-dataset show ufos

#+end_src

- *NOTA*: Si algo salió mal, puedes borrar el dataset con
#+begin_src shell :eval never
kite-dataset delete ufos

#+end_src


* Ejercicio III

- En este momento, tienes 3 veces los datos en tres formatos diferentes:
   1. Archivo de texto
   2. Archivo =avro=
   3. Guardado como tabla en =hive= y sus metadatos en el =hive metastore=.

- Más adelante veremos en detalle las /abstracciones/ y /procesadores/ que tiene =Hadoop= para manipular y analizar los datos, pero por el momento los usaremos para ver los datos, sin dar mucha explicación.
  - En lo que sigue, observa el código, todo tendrá más sentido cuando expliquemos apropiadamente estas herramientas.

- En este ejercicio, veremos =pig=, =hive= e =impala=.

#+REVEAL: split

- =Pig= es una abstracción sobre MapReduce

- =Pig= tiene un archivo de configuración localizado en =~/.pigbootup=

- Más adelante requeriremos algunos =JARs= para ejecutar cosas en =Pig=, en lugar de usarlos desde el sistema de archivos local, los leeremos desde el =hdfs=.
  - Crea una carpeta llamada =lib= en =/user/itam=
  - Copia a esta carpeta los siguientes archivos:
    - =/usr/lib/pig/datafu-1.1.0-cdh5.4.0.jar=
    - =/usr/lib/pig/piggybank.jar=
    - =/usr/lib/pig/lib/avro-1.7.6-cdh5.4.0.jar=
    - =/usr/lib/pig/lib/snappy-java-1.0.5.jar=
    - =/usr/lib/pig/lib/json-simple-1.1.jar=

#+REVEAL: split

- Crea el archivo =.pigbootup= en tu carpeta =$HOME= (i.e. =/home/itam=)

- Agrega lo siguiente:
#+begin_src shell :eval never
REGISTER hdfs://localhost/user/itam/lib/datafu-1.1.0-cdh5.4.0.jar
REGISTER hdfs://localhost/user/itam/lib/piggybank.jar
REGISTER hdfs://localhost/user/itam/lib/avro-1.7.6-cdh5.4.0.jar
REGISTER hdfs://localhost/user/itam/lib/snappy-java-1.0.5.jar
REGISTER hdfs://localhost/user/itam/lib/json-simple-1.1.jar

#+end_src

#+REVEAL: split

- Para ejecutarlo

#+begin_src shell :eval never
pig -useHCatalog

#+end_src


#+REVEAL: split

#+begin_src shell :eval never
ufos_dic = LOAD 'experimentos/UFO-Dic-2014.tsv' using PigStorage('\t')  \
           AS (Timestamp:chararray, \
               City:chararray, State:chararray, \
               Shape:chararray, Duration:chararray, \
               Summary:chararray, Posted:chararray);
DESCRIBE ufos_dic;
head = LIMIT ufos_dic 5;
DUMP head;

#+end_src

-  Puedes seguir la ejecución vía web  [[http://0.0.0.0:8088][aquí]].

- Nota el uso de mayúsculas para las palabras clave de =Pig=.

#+REVEAL: split

- Ahora usemos los archivos con formato =avro= y observemos como, dado que tienen metadatos, es mucho más fácil.
  - Nota lo limpio que va a quedar el código ahora...

#+begin_src shell :eval never
ufos = LOAD 'datasets/ufos' USING org.apache.pig.piggybank.storage.avro.AvroStorage();
DESCRIBE ufos;
ILLUSTRATE ufos;
head = LIMIT ufos 5;
DUMP head;

#+end_src

- Observa como no hubo problemas con el header del archivo!
  - ¡En el ejercicio anterior (tanto con =pig= como con =spark=) era la primera línea!

- Para ver los diferentes estados
#+begin_src shell :eval never
states = DISTINCT (FOREACH ufos GENERATE State);
DUMP states;

#+end_src

- Para salir presiona =Ctrl+C= ó =Ctrl+D=.

#+REVEAL: split

- Por último usaremos las herramientas parecidas a =SQL= que proveé Hadoop: =Hive= e =Impala=.

- Usaremos el =Hive Metastore=.
  - Aunque podríamos usar el =hdfs= o =avro= en el =hdfs=.

- Para ejecutar el cliente de =Hive=

#+begin_src shell :eval never
beeline -u jdbc:hive2://localhost:10000

#+end_src

- Veámos que tablas hay disponibles

#+begin_src shell :eval never
show tables;

#+end_src

#+REVEAL: split

- Obtengamos los primeros 5

#+begin_src shell :eval never
select * from ufos limit 5;

#+end_src

- Contar los estados diferentes:

#+begin_src shell :eval never
select count(distinct State) from ufos;

#+end_src

#+REVEAL: split

- Ver el plan de ejecución del /query/

#+begin_src shell :eval never
explain select count(distinct State) from ufos;

#+end_src


- Compara con este /query/
  - ¿Cuál es la diferencia?

#+begin_src shell :eval never
explain select count(*) from (select distinct State from ufos) as t;

#+end_src

- Para salir presiona =Ctrl+C= ó =Ctrl+D=.

#+REVEAL: split

- Para iniciar =Impala=

#+begin_src shell :eval never
impala-shell

#+end_src

- Debido a que Impala *no* es una abstracción de *MapReduce*, sus tiempos son impresionantemente rápidos

#+begin_src shell :eval never
invalidate metadata; # Siempre ejecutarlo cuando se modifiquen las tablas fuera de Impala
show tables;
describe ufos;
select * from ufos limit 5; # Este quizá tarde un poco... (warming up)
select * from ufos limit 15; # Debería de volar

#+end_src

- Top 5 de avistamientos por estado

#+begin_src shell :eval never
select state, count(*) as conteo from ufos group by state order by conteo desc limit 5;

#+end_src

- Para salir presiona =Ctrl+D=.

#+REVEAL: split

- Vimos diferentes maneras de interactuar con los datos
  - Lo vamos a profundizar luego.

- Es importante notar que aunque usamos diferentes herramientas para cada tipo de archivo (Texto, Avro, Tabla),  /todas/ las herramientas pueden ver /todos/ los formatos.
  - Casi...por lo menos los mostrados aquí.
  - Por ejemplo, podemos usar =pig= para leer las tablas de =hive=, cambiando el =LOAD= como sigue:

#+begin_src shell :eval never
ufos = load 'ufos' using org.apache.hive.hcatalog.pig.HCatLoader();
describe ufos;
illustrate ufos;
...
#+end_src

- Es importante notar también, que cada herramienta es para un diferente proceso (ingeniería, analítica, etc.)

- Estamos explorando los datos, aún no establecemos un /workflow/
  - También lo veremos más adelante.

* /Namenode/

- Hadoop proveé de dos formas de aliviar esta situación:
  - Respaldos: Se puede configurar al /namenode/ para que escriba su estado a varios /filesystems/.
  - /Secondary Namenode/

** /Namenode/

[[file:./imagenes/Selección_004.png]]


** /Datanode/
  - Lee y escribe los =HDFS= /blocks/ y los convierte en archivos del *FS* local.
  - Se comunica con otros /datanodes/ para la replicación de los datos.
  - Pueden realizar /caching/ de bloques.

** /Datanode/

[[file:./imagenes/Selección_005.png]]

** /Secondary Name Node/
  - Como el /namenode/ sólo hay uno por /cluster/.
  - No es un /namenode/.
  - Evita que el =edit log= crezca mucho.
  - No recibe ni guarda cambios en tiempo real del =HDFS=.
    - Va atrás del /namenode/.
  - Sólo toma /snapshots/ de la metadata.


** Línea de comandos

- Hay muchas maneras de conectarse y usar el =HDFS=. La línea de comandos es una de ellas.
  - Y espero que ya sepan que es de las más útiles y eficientes.

- Ayuda: =hadoop fs -help=

** Línea de comandos

#+begin_src shell :eval never

hadoop fs -cmd <args>
hadoop fs -ls
hadoop fs -mkdir
hadoop fs -copyFromLocal
hadoop fs -copyToLocal
hadoop fs -put archivo archivo_hdfs
hadoop fs -get archivo_hdfs
hadoop fs -cat archivo_hdfs
hadoop fs -cat archivo_hdfs head
hadoop fs -tail archivo_hdfs
hadoop fs -rm archivo_hdfs

#+end_src


* Flume

Flume is for high-volume ingestion into Hadoop of event-based data
	e.g collect logfiles from a bank of web servers, then 		move log events from those files to HDFS (clickstream)

* Sqoop

Open source tool to extract data from structured data store into Hadoop

Sqoop schedules map reduce jobs to effect imports and exports
Sqoop always requires the connector and JDBC driver
Sqoop requires JDBC drivers for specific database server, these should be copied to /usr/lib/sqoop/lib
The command-line structure has the following structure

#+BEGIN_EXAMPLE shell
sqoop TOOL PROPERTY_ARGS SQOOP_ARGS
#+END_EXAMPLE

TOOL  - indicates the operation that you want to perform, e.g import, export etc
PROPERTY_ARGS - are a set of parameters that are entered as Java properties in the format -Dname=value.
SQOOP_ARGS - all the various sqoop parameters.

* Sqoop

#+BEGIN_EXAMPLE
sqoop import \
--connect jdbc:oracle:thin:@devdb11-s.cern.ch:10121/devdb11_s.cern.ch \
--username hadoop_tutorial \
-P \
--num-mappers 1 \
--target-dir visitcount_rfidlog \
--table VISITCOUNT.RFIDLOG
#+END_EXAMPLE

* Sqoop: Paralelización

#+BEGIN_EXAMPLE shell

-- table table_name

-- query select * from table_name where $CONDITIONS

-- table table_name
-- split-by primary key
-- num-mappers n

-- table table_name
-- split-by primary key
-- boundary-query select range from dual
-- num-mappers n
#+END_EXAMPLE


* Ejercicio I

Use Kite SDK to demonstrate copying of various file formats to Hadoop

Step 1) Download the MovieLens Dataset

#+BEGIN_EXAMPLE
curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o movies.zip 
unzip movies.zip
cd ml-latest-small/
#+END_EXAMPLE

Step 2) Load the Dataset into Hadoop in Avro format

#+BEGIN_EXAMPLE
-- infer the schema
kite-dataset csv-schema ratings.csv --record-name ratings -o ratings.avsc
cat ratings.avsc
-- create the schema
kite-dataset create ratings --schema ratings.avsc
-- load the data
kite-dataset csv-import ratings.csv --delimiter ',' ratings
#+END_EXAMPLE

Step 3) Load the Dataset into Hadoop in Parquet format

#+BEGIN_EXAMPLE
-- infer the schema
kite-dataset csv-schema ratings.csv --record-name ratingsp -o ratingsp.avsc
cat ratingsp.avsc
-- create the schema
kite-dataset create ratingsp --schema ratingsp.avsc --format parquet
-- load the data
kite-dataset csv-import ratings.csv --delimiter ',' ratingsp
#+END_EXAMPLE


Step 4) Run a sample query to compare the elapsed time between Avro & Parquet

#+BEGIN_EXAMPLE
hive
select avg(rating)from ratings;
select avg(rating)from ratingsp;
#+END_EXAMPLE

* Ejercicio II

Use Sqoop to copy an Oracle table to Hadoop

Step 1) Get the Oracle JDBC driver

#+BEGIN_EXAMPLE

#+END_EXAMPLE

Step 2) Run the sqoop job
#+BEGIN_EXAMPLE
sqoop import \
--connect jdbc:oracle:thin:@devdb11-s.cern.ch:10121/devdb11_s.cern.ch \
--username hadoop_tutorial \
-P \
--num-mappers 1 \
--target-dir visitcount_rfidlog \
--table VISITCOUNT.RFIDLOG
#+END_EXAMPLE

* Ejercicio III

Use Sqoop to copy an Oracle table to Hadoop, multiple mappers

#+BEGIN_EXAMPLE
sqoop import \
--connect jdbc:oracle:thin:@devdb11-s.cern.ch:10121/devdb11_s.cern.ch \
--username hadoop_tutorial \
-P \
--num-mappers 2 \
--split-by alarm_id \
--target-dir lemontest_alarms \
--table LEMONTEST.ALARMS \
--as-parquetfile
#+END_EXAMPLE

Check the size and number of files

#+BEGIN_EXAMPLE
hdfs dfs -ls lemontest_alarms/
#+END_EXAMPLE

* Ejercicio IV

Use Sqoop to make incremental copy of a Oracle table to Hadoop

Step 1) Create a sqoop job

#+BEGIN_EXAMPLE
sqoop job \
--create alarms \
-- \
import \
--connect jdbc:oracle:thin:@devdb11-s.cern.ch:10121/devdb11_s.cern.ch \
--username hadoop_tutorial \
-P \
--num-mappers 1 \
--target-dir lemontest_alarms_i \
--table LEMONTEST.ALARMS \
--incremental append \
--check-column alarm_id \
--last-value 0 \
#+END_EXAMPLE

Step 2) Run the sqoop job

#+BEGIN_EXAMPLE
sqoop job --exec alarms
#+END_EXAMPLE


Step 3) Run sqoop in incremental mode

#+BEGIN_EXAMPLE
sqoop import \
--connect jdbc:oracle:thin:@devdb11-s.cern.ch:10121/devdb11_s.cern.ch \
--username hadoop_tutorial \
-P \
--num-mappers 1 \
--table LEMONTEST.ALARMS \
--target-dir lemontest_alarms_i \
--incremental append \
--check-column alarm_id \
--last-value 47354 \

#+END_EXAMPLE

#+BEGIN_EXAMPLE
hdfs dfs -ls lemontest_alarms_i/
#+END_EXAMPLE

* YARN
:PROPERTIES:
   :reveal_background: #000fff
 :END:

** YARN

- La infraestructura de Hadoop =0.x= y =1.x= era monolítica, por eso fue rediseñada.
- =YARN=: /Yet Another Resource Negotiator/.
- La gestión de recursos es extraída de los paquetes de =MapReduce= para que puedan ser utilizadas por otros componentes.
- Aportaciones
  - Escalabilidad.
  - Compatibilidad con =MapReduce=.
  - Mejoras en la gestión del /cluster/.
  - Soporte para otros modelos de programación (además de =MapReduce=).
    - /Graph processing/
    - /Message Passing Interface/ (*MPI*).
    - Soporte para procesamiento /real-time/ o /near real-time/.
      - =MapReduce= es /batch-oriented/.
  - Agilidad.

** YARN

- Se dividieron las dos responsabilidades del /JobTracker/:
  - Gestión de recursos (/Resource Management/)
  - Asignación y vigilancia de trabajos (/Job scheduling-monitoring/)

- La idea es tener un /ResourceManager/ global y un /NodeManager/ por
  nodo esclavo, los cuales forman un sistema para la administración de
  aplicaciones distribuidas.

- El /ResourceManager/ tiene dos componentes principales:
  - /Scheduler/: Asigna los recursos para las aplicaciones (/pluggeable/).
  - /Application Manager/: Responsable de aceptar las solicitudes de
    trabajos, negociando al principio para ejecutar el /Application
    Master/ específico y provee un servicio de reinicio, por si el
    /Application Master/ falla.

#+REVEAL: split

- En cada nodo:

   - El /Application Master/: Negocia sus recursos con el /Scheduler/,
  monitorea sus avances y reporta su estatus.

   - El /NodeManager/ es el responsable de los contenedores,
     monitorear el uso de recursos y reportar todo al
     /ResourceManager/.

** Arquitectura MapReduce Hadoop 1.x

[[file:./imagenes/MRArch.png]]

** Arquitectura Hadoop 2.x

[[file:./imagenes/Selección_003.png]]


** Cambios 1.x -> 2.x

[[file:./imagenes/yarn.png]]


** Multiparadigma en Hadoop 2.x

[[file:imagenes/Hadoop-2.0-Intro-Blog2.jpg]]



Imagen tomada de [[http://www.edureka.co/blog/apache-hadoop-2-0-and-yarn/][edureka!]]

** Procesadores y Abstracciones

[[file:imagenes/HDFS.jpg]]


Imagen tomada de [[http://radar.oreilly.com/2015/02/processing-frameworks-for-hadoop.html][O'reilly]]


* Procesamiento

* Tipos

- MapReduce
- Spark
- Impala


* Procesamiento: MapReduce

* MapReduce en Hadoop
- Principal /framework/ de ejecución de =Apache Hadoop=.
- Inspirado en las operaciones *MAP* y *REDUCE* de los lenguajes funcionales.
- Modelo de programación para proceso de datos distribuido  y paralelo.
- Divide las tareas (/jobs/) en fases de /mapeo/ y fases de /reducción/.
- Los desarrolladores crean tareas /MapReduce/ para Hadoop usando datos guardados en el =HDFS=.

* MapReduce: Ventajas

  - /Fault-tolerant/.
  - Esconde los detalles de implementación a los programadores.
  - Escala con el tamaño de los datos.


* MapReduce

- Dos fases de procesamiento:
  - /key-value/ como Input y Output
  - El programador especifica:
    - Tipos de /key-value/
    - Funciones: =MAP= y =REDUCE=.


* Una pequeña regresión...

* map-reduce: Matemáticamente

#+BEGIN_SRC SHELL :EVAL NEVER
map: (k1, v1) -> list(k2, v2)

#+END_SRC

- =map= Mapea (aplica una función /f/) un conjunto de entrada de pares /key-value/ a otro conjunto intermedio de /key-values/


* map-reduce: Matemáticamente

#+BEGIN_SRC SHELL :EVAL NEVER
reduce: (k2, list(v2)) -> list(k3, v3)

#+END_SRC

- =reduce=  Aplica una función /g/ a todos los valores (/values/) asociados a una llave (/key/) y acumula el resultado. Emite pares de /key-values/.

* Python =map=

#+begin_src python :results output :export both
# Equivalente en for-loop

items = [1,2,3,4,5]
cuadrados = []
for x in items:
    cuadrados.append(x**2)

print cuadrados
#+end_src

#+RESULTS:
: [1, 4, 9, 16, 25]


#+begin_src python :results output :export both
# Usando la funcion map(function, sequence)

items = [1,2,3,4,5]

print list(map((lambda x: x**2), items))
#+end_src

#+RESULTS:
: [1, 4, 9, 16, 25]


* Python =reduce=

#+begin_src python :results output :export both
# Equivalente en for-loop
L = [1,2,3,4]
result = L[0]
for x in L[1:]:
    result = result*x

print result

#+end_src

#+RESULTS:
: 24

#+begin_src python :results output :export both
# Usando la funcion reduce(funcion, secuencia)
print reduce((lambda x,y: x*y), [1,2,3,4])
#+end_src

#+RESULTS:
: 24

* Python =map= y =reduce=

#+begin_src python :results output :export both
a = range(1, 4)
b = range(4, 9)
c = range(9, 15)
print "a ->  %s, b -> %s , c -> %s" % (a, b, c)

L1 = map(lambda x:len(x), [a,b,c])
print "L1 -> %s" % L1

L2 = reduce(lambda x, y: x+y, L1)
print "L2 -> %s" % L2
#+end_src

#+RESULTS:
: a ->  [1, 2, 3], b -> [4, 5, 6, 7, 8] , c -> [9, 10, 11, 12, 13, 14]
: L1 -> [3, 5, 6]
: L2 -> 14



* MapReduce y map-reduce

- Básicamente es lo mismo, pero...
- =map=, =reduce= (entre otras) son parte de lenguajes funcionales.
- =MapReduce= es la aplicación de esta idea aplicada a problemas /vergonzosamente/ /paralelos/.
  - Ver la carpeta =docs= para el artículo de *Google* sobre =MapReduce=.


* GNU Parallel

#+begin_src shell
find ./data/books -type f | parallel -j0 egrep -i  '\[\[:digit:\]\]' {} | awk '{s+=$1} END {print s}'
#+end_src

#+RESULTS:
: 1480715


- *¿Puedes identificar las partes =map= y =reduce=?*
- Esto ya es un =MapReduce=.


* MapReduce en Hadoop

- A nivel programático:
  - /Data/ de entrada
  - Programa MapReduce
  - Configuración
  - Subtareas: =map= y =reduce=


* MapReduce: /Mapper/

- Hadoop divide la entrade de datos al /job/ MapReduce en pedazos de tamaño fijo llamados /input splits/.
- Hadoop crea una tarea =map= para cada /input split/.
- =map= escribe al /file system/ local.
  - Si el =reducer= tiene éxito se borra la salida del /mapper/.

* Map only

[[file:./imagenes/map_only.png]]


* MapReduce: /Reducer/

- La entrada es la salida de (posiblemente) todos los /mappers/.
- Estas se transmiten vía red al nodo donde corre el /reducer/.
- La salida se guarda en el =HDFS=.

* Map, One reduce

[[file:./imagenes/map_one_reduce.png]]

* MapReduce

[[file:./imagenes/map_reduce.png]]


* MapReduce: /Combiner/

- Es una medida de optimización.
- Es para ahorrar ancho de banda.
- Una especie de /reducer/ local.
- No es parte (estrictamente) del MapReduce
  - Por eso no lo había mencionado.


* Word count

- Es el ejemplo /Hola Mundo/ de Apache Hadoop.
- No sólo eso, es el ejemplo que se utiliza en el trabajo seminal
  - *MapReduce: Simplified Data Processing on Large Clusters* /(2006)/.
  - En la carpeta =docs= como ya había dicho.
- Solamente 1 =Map= y 1 =Reduce=.

* Word count

- *mapper*
  - =k1= -> nombre de archivo
  - =v1= -> texto del archivo
  - =k2= -> palabra
  - =v2= -> "1"

- *reducer*
  - =k2= -> palabra
  - list(v2) -> (1,1,1,1,1,1,..., 1)

  Suma los "1" y produce una lista de

  - k3 -> palabra
  - v3 -> suma

* Word count

[[file:./imagenes/word_count.png]]

* Pseudocódigo

#+begin_src shell :eval never
map (String key, String value)
   for each word w in value
      Emit(w, 1)

reduce (String key, Iterator values)
   int wordcount = 0
   for each v in values
      wordcount += v
      Emit(key, wordcount)


#+end_src

* Mockup

- Ver los archivos =word_count.py= y =mapreduce.py= en la carpeta =mock=.

#+begin_src shell :eval never
chmod +x word_count.py
python word_count.py

#+end_src

- Este es un ejemplo de mentiritas, no usa Apache Hadoop.


* Ejercicio


- Diseñe (en /pseudo código/, imágen, código, lo que sea más fácil) el **MapReduce** para lo siguiente:
  - Encontrar el máximo de un conjunto de datos.
  - Encontrar el promedio y desviación estándar de unos datos.
  - Encontrar el top 10 de una cantidad.
  - Contar por grupo


* Abstracciones de MapReduce

* Abstracciones: Pig
* Pig

- Proyecto de Apache
- Abstracción encima de Hadoop
  - /Pig Latin/ compila a =MapReduce=
  - En cierta forma /Pig Latin/ es para analistas, /data scientist/ y estadísticos.
  - =MapReduce=  es para programadores (aunque los /data scientist/ deberían de poder hacerlo también)

* Pig

- Pig es un /data flow programming language/
- Es decir,
  - Ejecuta paso a paso
  - Cada paso es una transformación de datos
- En cambio =SQL= es un conjunto de /constraints/ que en conjunto definen el resultado buscado.

* Pig

- ¿Qué cosas puede hacer?
  - =joins=
  - =sorts=
  - =filters=
  - =group by=
  - /User defined functions/ =UDF='s

* Pig

- ¿Qué cosas *puedo* hacer?

  - =ETLs=
    - Limpiar.
    - /Joins/ gigantes.

  - Búsqueda en /Raw/.

* Pig

- Componentes
  - /Pig Latin/
    - Los =keywords= no son /case-sensitive/, pero las relaciones y los =UDFs= si lo son.
  - =Grunt=
    - Local
    - MapReduce
  - =Pig compiler=

* Pig

- Es posible ejecutar también /scripts/ de /Pig Latin/ (terminación =.pig=) sin entrar a =grunt=.

#+begin_src shell :eval never
pig script_file.pig

#+end_src

- Si quieren pasar parámetros
#+begin_src shell :eval never
pig -p var=bla/bla var2=bla/bla/bla script_file.pig

#+end_src

- Y usarse desde programas en =Java= con la clase =PigServer=.
  - Como una especie de =JDBC=, pero para /Pig Latin/.


* Pig latin: /Building blocks/

- Escalares
  - Son interfaces a clases =java.lang=
    - =int=, , =long=, =float=, =double=
  - Por ejemplo:
#+begin_src shell :eval never
'Adolfo'

#+end_src

- Tuplas
  - Colleción ordenada de tamaño fijo de datos.
  - Están divididos en /fields/, cada uno conteniendo un elemento.
  - Como son ordenados, se pueden referir por posición.
#+begin_src shell :eval never
('Adolfo', 3, 8.17, 23)

#+end_src

- /Bags/
  - Colección sin ordenar de tuplas.
#+begin_src shell :eval never
{('Adolfo', 3, 8.17, 23), ('Paty', 3.14, 9, 'A')}

#+end_src


* Pig latin: /Operaciones/

- =load=, =store=, =dump=
#+begin_src shell :eval never
store procesados into 'output/procesados'; -- Guarda la relación en el  HDFS
dump procesados; -- Imprime en pantalla la relación

#+end_src

- =foreach=
  - Aplica un conjunto de expresiones a cada elemento del /data pipeline/.
  - Es el operador de proyección de =Pig latin=.

- =filter=
  - Seleccionar que registros se mantendrán en el /data pipeline/.

- =group=
  - Agrupa registros con la misma llave en un /bag/.
  - La sintaxis es parecida a la de =sql=, pero son muy diferentes.
    - No hay relación entre el agrupamiento y las funciones de agregación (recuerden sus clases de =sql=).

#+REVEAL: split

- =order by=
  - Ordena los datos.

- =distinct=
  - Remueve duplicados.

- =limit=, =sample=
  - Limita la cantidad de información que se ve.

- =parallel=
  - Afecta la cantidad de =reducers= que hay.


* ¿Qué funciones hay?

[[http://www.qubole.com/resources/cheatsheet/pig-function-cheat-sheet/][Pig Cheatsheet]]


* Expresiones Regulares de Java

- En los siguientes ejercicios llegaremos a usar expresiones regulares =Pig= y =hive= soportan las =regex= de =Java=, [[http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html][aquí]] pueden obtener más información sobre el tema.

* Ejemplo: Wordcount

- Para comprender bien lo que está pasando te recomiendo usar =illustrate= o =describe= en cada paso.

#+begin_src shell :eval never
shakespeare = load 'books/pg100.txt' using TextLoader as (line:chararray);
-- Usando UDFs y expresiones regulares de Java
palabras = foreach shakespeare generate flatten(TOKENIZE(REPLACE(LOWER(TRIM(line)), '[\\p{Punct}, \\p{Cntrl}]', ' '))) as palabra;
grupo = group palabras by palabra;
conteo = foreach grupo generate $0 as palabra, count($1) as cantidad;
ordenados = order conteo by cantidad desc;
top10 = limit ordenados 10;
dump top10;

#+end_src

* Ejercicio
- Describe cada línea con comentarios y agrega los esquemas.
  - i.e. elimina los =$0=, etc
- Guarda la salida de cada uno de estos en una carpeta =output/wordcount/pig= en tu carpeta =hdfs=.
- Copia la salida al sistema de archivos local y súbelo a =github=.

* Pig: JOINS

- Cargamos fuente 1
- Cargamos fuente 2
- Unimos las fuentes (/bags/) mediante una llave
- Súper simple

- Pig soporta /inner joins/ (valor por omisión), /left outer joins/ (y /right/ también) y /full outer/ joins.


#+begin_src shell :eval never
fuentes_unidas = join fuente1 by (keys) [left|right|full outer] fuente2 by (keys);

#+end_src

- Además =Pig= soporta =cogroup= además de los =joins= (el =cogroup= preserva la estructura de las fuentes y crea tuplas por cada llave)

#+begin_src shell :eval never
fuentes_unidas = cogroup fuente1 by (keys) fuente2 by (keys);

#+end_src


* Pig: Ejemplo de JOINs y COGROUPs

- Fuente de datos: =mascotas (dueño, mascotas)=
#+begin_src shell :eval never
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)

#+end_src

- Fuente de datos: =amigos(amigo1, amigo2)=
#+begin_src shell :eval never
(Diana, Adolfo)
(Gabriel, Adolfo)
(Shanti, Paty)

#+end_src

#+REVEAL: split

- =COGROUP mascotas by dueño, amigos por amigo2;=
#+begin_src shell :eval never
(Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)}, {(Diana, Adolfo), (Gabriel, Adolfo)})
(Paty, {(Paty, perro), (Paty, gato)}, {(Shanti, Paty)})

#+end_src

- =JOIN mascotas by dueño, amigos por amigo2;=
#+begin_src shell :eval never
(Adolfo, tortuga, Diana)
(Adolfo, tortuga, Gabriel)
(Adolfo, pez, Diana)
(Adolfo, pez, Gabriel)
(Adolfo, gato, Diana)
(Adolfo, gato, Gabriel)
(Paty, perro, Shanti)
(Paty, gato, Shanti)

#+end_src

* Aclaraciones sobre GROUP y FLATTEN


- =FLATTEN= elimina un nivel anidamiento
  - Ejemplo
#+begin_src shell :eval never
(Adolfo, (tortuga, pez, gato))
(Paty, (perro, gato))

#+end_src
  - FLATTEN eliminaría los bags internos
#+begin_src shell :eval never
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)

#+end_src

- =GROUP .. BY= organiza los /bags/ en /bags/
  - Siguiendo con los datos anteriores de mascotas:
    - GROUP mascotas BY dueño;
#+begin_src shell :eval never
( Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)} )
( Paty, {(Paty, perro), (Paty, gato)} )

#+end_src

- En cierto sentido =FLATTEN= y =GROUP .. BY= son operaciones inversas
  entre sí.

* Ejercicio

- Describe cada línea con comentarios y agrega los esquemas.
- Guarda la salida de cada uno de estos en una carpeta =output/ufos/pig= en tu carpeta =hdfs=.
- Copia la salida al sistema de archivos local y súbelo a =github=.

#+begin_src shell :eval never
ufos = load 'ufos' using org.apache.hive.hcatalog.pig.HCatLoader();
a_imprimir = limit ufos 5;
por_estado = group ufos by State;
describe por_estado;
explain por_estado;
illustrate por_estado;
-- itera sobre cada elemento del bag
conteo = foreach por_estado generate group count_star(ufos);
ordenados = order conteo by $1 desc;
top_five = limit ordenado 5;
unicos = distinct conteos;
muestreo = sample por_estado 0.1;
filtrados = filter conteos by substring(group, 0, 2) == 'W';
mayores = filter conteos by $1 > 50;

#+end_src


* Abstracciones: Hive


* Hive

- Proyecto de Apache.
- _Abstracción_ pra modelar y procesar datos en Hadoop.
- Proveé de una manera de estructurar datos guardados en el =HDFS=.
- Permite crear _queries_ muy similares a =SQL= (llamado =HQL=) y correrlos contra los datos.
- Contiene un almacén de metadatos (=HCatalog=), que además puede ser compartido con otras interfaces como =Pig=, =MapReduce=, =Impala=, =Spark=, etc.
- Da Acceso al =HDFS= y =HBase=.

* Bibliografía recomendada

- Sitio web de Hive
- Hadoop: The Definitive Guide
- Programming Hive


* Arquitectura de Apache Hive

[[file:./imagenes/hive-remote.jpeg]]


* ¿Qué funciones hay?

[[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF][Apache Hive Docs]]

* Ejemplo: Wordcount

- Describe cada línea con comentarios y agrega los esquemas.
- Averigua (usando la documentación) como guardar la tabla a archivo y compara el =top10= con el resultado de =Pig=.
- ¿Por qué da diferente?
- Modifica el código para arreglar la diferencia.
- Guarda la salida de cada uno de estos en una carpeta =output/wordcount/hive= en tu carpeta =hdfs=.
- Copia la salida al sistema de archivos local

#+REVEAL: split

#+begin_src shell :eval never
-- Limpiamos el ambiente
delete table shakespeare;
delete table wordcount;

-- Creamos la tabla que contendrá las obras de Shakespeare
create table shakespeare(linea string);

-- Verifiamos
show tables;

-- Cargamos los datos a la tabla
load data inpath '/user/itam/books/pg100.txt' overwrite into table shakespeare;
#+end_src


#+REVEAL: split

#+begin_src shell :eval never

-- Quereamos y guardamos en una tabla
-- Los símbolos raros '\\p{Punct}' y similares son expresiones regulares de Java
create table wordcount as
select palabra, count(*) as conteo from
(
select
explode(split(lcase(regexp_replace(trim(linea),'[\\p{Punct}, \\p{Cntrl}]', ' ')), ' ')) as palabra
from shakespeare
) palabras
group by palabra
order by conteo desc limit 10;
-- Este código  se podría hacer más pequeño con LATERAL (¿Recuerdan la clase de PostgreSQL?)

#+end_src

* Ejercicio

- Repite el ejercicio de =Pig= sobre =ufos=, pero ahora en =Hive=.



* Procesamiento: Spark

* Ejemplo: WordCount

#+begin_src shell :eval never

def tokenize(texto):
    texto.split()

shakespeare = sc.textFile("hdfs://localhost/user/itam/books/pg100.txt")

wordcount = shakespeare.flatMap(tokenize).\
                        map(lambda x: (x,1)).\
                        reduceByKey(add).\
                        map(lambda x: (x[1], x[0])).\
                        sortByKey(False)

wordcount.take(10)

#+end_src

* Ejercicio
- Explica el código anterior
- Modifica el código de =Spark= y =Python= para que reproduzca el resultado de =Pig= y =Hive=.

* Ejercicio

- Repite el ejercicio de =Pig= sobre =ufos=, pero ahora en =Spark=.


* Procesamiento: Impala

* Impala
- Cloudera basó este desarrollo en dos /white papers/ de Google describiendo /baja latencia/ en consultas con tecnologías llamadas *F1* y *Dremel*.
- No está basado en el motor de procesamiento =MapReduce=.
- Optimizado en /latencia/.
- Usa =SQL= y utiliza =Hive Metastore=.
- Soporta el =hdfs= y =HBase=.

* Impala

- Todo está en memoria
  - No escribe a disco como =MapReduce=.
- Tiene los demonios siempre corriendo.
  - No levanta procesos para cada tarea, como =MapReduce=.
- Escrito en =C++= no en =Java=.

- *Nota*: Con la aparición de [[https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark:+Getting+Started?cmp=ex-data-na-na-na_architectural_considerations_for_hadoop_applications_2][=Hive on Spark=]], habrá que ver cual /framework/ gana.

* ¿Qué funciones hay?

[[http://www.cloudera.com/content/cloudera/en/documentation/cloudera-impala/latest/topics/impala_functions.html][Built-in Functions de Impala]]

* Ejercicio

- Escribe el wordcount en Impala.
  - Deberás averiguar que funciones utilizar para reproducir la salida de =Pig= y =Hive=.

- Súbelo a github el código y el resultado, así como la comparación con las otras salidas.

* Ejercicio

- Repite el ejercicio de =Pig= sobre =ufos=, pero ahora en =Impala=.




* Disclaimer
:PROPERTIES:
   :reveal_background: #e95d3c
:END:
- Algunas imágenes se tomaron de los libros /Professional Hadoop Solutions/ de *Wrox* y de la página de [[http://hortonworks.com/hadoop/yarn/][*Hortonworks*]]. Las otras son mías.
- Debería de ser claro cuales son cuales. =(^_^)=
- Para otras imágenes, en la lámina se indica de dónde fueron tomadas.




* COMMENT Settings
# Local Variables:
# org-babel-sh-command: "/bin/bash"
# org-confirm-babel-evaluate: nil
# org-export-babel-evaluate: nil
# ispell-check-comments: exclusive
# ispell-local-dictionary: "spanish"
# End:
